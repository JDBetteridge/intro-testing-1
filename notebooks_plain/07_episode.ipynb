{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Running tests with pytest"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Overview\n- **Teaching:** 10 min\n- **Exercises:** 5 min\n\n**Questions**\n- How do I automate my tests?\n\n**Objectives**\n- Understand how to run a test suite using the pytest framework\n- Understand how to read the output of a pytest test suite"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "We created a suite of tests for our mean function, but it was annoying to run them one at a time. It would be a lot better if there were some way to run them all at once, just reporting which tests fail and which succeed.\n\nThankfully, that exists. Recall our tests are in a file called `test_mean.py`:\n\n```python\nfrom mean import *\n\ndef test_ints():\n    num_list = [1,2,3,4,5]\n    obs = mean(num_list)\n    exp = 3\n    assert obs == exp\n\ndef test_zero():\n    num_list=[0,2,4,6]\n    obs = mean(num_list)\n    exp = 3\n    assert obs == exp\n\ndef test_double():\n    # This one will fail in Python 2\n    num_list=[1,2,3,4]\n    obs = mean(num_list)\n    exp = 2.5\n    assert obs == exp\n\ndef test_long():\n    big = 100000000\n    obs = mean(range(1,big))\n    exp = big/2.0\n    assert obs == exp\n\ndef test_complex():\n    # given that complex numbers are an unordered field\n    # the arithmetic mean of complex numbers is meaningless\n    num_list = [2 + 3j, 3 + 4j, -32 - 2j]\n    obs = mean(num_list)\n    exp = NotImplemented\n    assert obs == exp\n```"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "The framework we will use to run our test suite is `pytest` which can be run from the directory containing the tests:\n```python\n%run pytest\n```\n```brainfuck\n================================================ test session starts ================================================\nplatform linux -- Python 3.6.3, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\nrootdir: /home/rjg20/training/arc-training/intro-testing, inifile:\ncollected 5 items\n\ntest_mean.py ....F\n\n===================================================== FAILURES ======================================================\n___________________________________________________ test_complex ____________________________________________________\n\n    def test_complex():\n        # given that complex numbers are an unordered field\n        # the arithmetic mean of complex numbers is meaningless\n        num_list = [2 + 3j, 3 + 4j, -32 - 2j]\n>       obs = mean(num_list)\n\ntest_mean.py:32:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nsample = [(2+3j), (3+4j), (-32-2j)]\n\n    def mean(sample):\n        '''\n        Takes a list of numbers, sample\n\n        and returns the mean.\n        '''\n\n        assert len(sample) != 0, \"Unable to take the mean of an empty list\"\n        for value in sample:\n>           assert isinstance(value,int) or isinstance(value,float), \"Value in list is not a number.\"\nE           AssertionError: Value in list is not a number.\n\nmean.py:12: AssertionError\n======================================== 1 failed, 4 passed in 14.90 seconds ========================================\n```"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## What did pytest do?\n\nIn the above case, the pytest package ‘sniffed-out’ the tests in the directory and ran them together to produce a report of the sum of the files and functions matching the regular expression `[Tt]est[-_]*`.\n\nThe major boon a testing framework provides is exactly that, a utility to find and run the tests automatically. With pytest, this is the command-line tool called `pytest`. When `pytest` is run, it will search all directories below where it was called, find all of the Python files in these directories whose names start or end with `test`, import them, and run all of the functions and classes whose names start with `test` or `Test`. This automatic registration of test code saves tons of human time and allows us to focus on what is important: writing more tests (and the code to pass them!)."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "When you run `pytest`, it will print a dot (`.`) on the screen for every test that passes, an `F` for every test that fails or where there was an unexpected error. In rarer situations you may also see an `s` indicating a skipped tests (because the test is not applicable on your system) or a `x` for a known failure (because the developers could not fix it promptly). After the dots, `pytest` will print summary information.\n\nWithout changing the tests, alter the mean.py file from the previous section until it passes. When it passes, py.test will produce results like the following:\n```bash\n%run pytest\n```\n```brainfuck\ncollected 5 items\n\ntest_mean.py .....\n\n========================== 5 passed in 2.68 seconds ===========================\n```"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "Also try running `pytest` with the *useful* flags `-s`, `-v`, and `-x`. Check the `pytest` manual to see what each of these do if you cannot determine it from their behaviour.\n\nAs we write more code, we would write more tests, and `pytest` would produce more dots. Each passing test is a small, satisfying reward for having written quality scientific software. Now that you know how to write tests, let’s go into what can go wrong."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Key Points\n- The `pytest` command collects and runs tests starting with Test or test_.\n- `.` means the test passed\n- `F` means the test failed or erred\n- `x` is a known failure\n- `s` is a purposefully skipped test"
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}